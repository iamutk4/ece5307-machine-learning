{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handcrafted Fully Connected Neural Net (FCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "Xtr_loadpath = 'Xtr.csv'\n",
    "Xts_loadpath = 'Xts.csv'\n",
    "ytr_loadpath = 'ytr.csv'\n",
    "\n",
    "Xtr = np.loadtxt(Xtr_loadpath, delimiter=\",\")\n",
    "Xts = np.loadtxt(Xts_loadpath, delimiter=\",\")\n",
    "ytr = np.loadtxt(ytr_loadpath, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model was tested with both standardization, and normalization. Normalization doesn't seem to \n",
    "# give good results, that's why we finalized our design with standardization. \n",
    "# standardize the training data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale= StandardScaler()\n",
    "\n",
    "Xtr_standardized = scale.fit_transform(Xtr) # revise this line as needed\n",
    "Xts_standardized = scale.fit_transform(Xts) # revise this line as needed\n",
    "ytr_standardized = ytr # revise this line as needed\n",
    "\n",
    "Xtr_torch = torch.Tensor(Xtr_standardized)\n",
    "Xts_torch = torch.Tensor(Xts_standardized)\n",
    "ytr_torch = torch.Tensor(ytr_standardized)\n",
    "\n",
    "# save the standardized training data\n",
    "Xtr_savepath = 'Xtr_pytorch.csv'\n",
    "Xts_savepath = 'Xts_pytorch.csv'\n",
    "ytr_savepath = 'ytr_pytorch.csv'\n",
    "yts_hat_savepath = 'yts_hat_pytorch.csv'\n",
    "\n",
    "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
    "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
    "np.savetxt(ytr_savepath, ytr_standardized, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split, and Creation of Pytorch Dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(Xtr_torch,ytr_torch,test_size=0.2,random_state = random.randint(0,1000))\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "test_ds =  TensorDataset(x_test, y_test)\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handcrafted Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (softmax): Softmax(dim=None)\n",
      "  (relu): ReLU()\n",
      "  (tanh): Tanh()\n",
      "  (batch_normal): BatchNorm1d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (Dense1): Linear(in_features=8, out_features=95, bias=True)\n",
      "  (Dense2): Linear(in_features=95, out_features=95, bias=True)\n",
      "  (Dense3): Linear(in_features=95, out_features=80, bias=True)\n",
      "  (Dense4): Linear(in_features=80, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a model\n",
    "d_in = Xtr.shape[1]\n",
    "d_out = 1\n",
    "nin = 8\n",
    "nh=100\n",
    "nout=1\n",
    "\n",
    "activation_fn = nn.Sigmoid()\n",
    "dropout_prob = 0.5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,nin,nh,nout):\n",
    "        super(Net,self).__init__()\n",
    "        self.sigmoid = activation_fn\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.batch_normal = nn.BatchNorm1d(95)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.Dense1 = nn.Linear(nin,95)\n",
    "        self.Dense2 = nn.Linear(95,95)\n",
    "        self.Dense3 = nn.Linear(95,80)\n",
    "        self.Dense4 = nn.Linear(80,nout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.sigmoid(self.Dense1(x))\n",
    "        x = self.dropout(x)\n",
    "#         x = self.batch_normal(x)\n",
    "        x = self.sigmoid(self.Dense2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.batch_normal(x)\n",
    "        x = self.sigmoid(self.Dense3(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.batch_normal(x)\n",
    "        out = self.sigmoid(self.Dense4(x))\n",
    "        return out\n",
    "\n",
    "model1 = Net(nin=nin, nh=nh, nout=nout)\n",
    "\n",
    "print(str(model1))\n",
    "\n",
    "# Usually, we would train the model at this point. \n",
    "# But this is only a demo, so we'll use the randomly initialized weights.\n",
    "# num_epoch = 40\n",
    "lrate = 2e-3\n",
    "# decay = lrate/num_epoch\n",
    "\n",
    "opt = optim.Adam(model1.parameters(), lr=lrate)\n",
    "\n",
    "# lambda1 = lambda epoch: (1-decay)*epoch\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1   Train Loss: 0.549   Train Accuracy: 73.15    Test Loss: 0.476   Test Accuracy: 78.05   Time: 0.51sec\n",
      "Epoch:  2   Train Loss: 0.484   Train Accuracy: 78.74    Test Loss: 0.474   Test Accuracy: 78.35   Time: 0.62sec\n",
      "Epoch:  3   Train Loss: 0.477   Train Accuracy: 78.85    Test Loss: 0.468   Test Accuracy: 78.05   Time: 0.59sec\n",
      "Epoch:  4   Train Loss: 0.471   Train Accuracy: 78.85    Test Loss: 0.456   Test Accuracy: 78.55   Time: 0.60sec\n",
      "Epoch:  5   Train Loss: 0.457   Train Accuracy: 79.11    Test Loss: 0.442   Test Accuracy: 78.70   Time: 0.52sec\n",
      "Epoch:  6   Train Loss: 0.447   Train Accuracy: 79.83    Test Loss: 0.435   Test Accuracy: 79.60   Time: 0.64sec\n",
      "Epoch:  7   Train Loss: 0.441   Train Accuracy: 79.97    Test Loss: 0.428   Test Accuracy: 80.25   Time: 0.65sec\n",
      "Epoch:  8   Train Loss: 0.440   Train Accuracy: 80.31    Test Loss: 0.425   Test Accuracy: 80.65   Time: 0.54sec\n",
      "Epoch:  9   Train Loss: 0.438   Train Accuracy: 80.29    Test Loss: 0.421   Test Accuracy: 80.90   Time: 0.69sec\n",
      "Epoch: 10   Train Loss: 0.433   Train Accuracy: 80.35    Test Loss: 0.422   Test Accuracy: 81.40   Time: 0.53sec\n",
      "Epoch: 11   Train Loss: 0.432   Train Accuracy: 80.78    Test Loss: 0.419   Test Accuracy: 80.85   Time: 0.78sec\n",
      "Epoch: 12   Train Loss: 0.430   Train Accuracy: 80.54    Test Loss: 0.420   Test Accuracy: 81.70   Time: 0.64sec\n",
      "Epoch: 13   Train Loss: 0.425   Train Accuracy: 81.10    Test Loss: 0.412   Test Accuracy: 82.30   Time: 0.76sec\n",
      "Epoch: 14   Train Loss: 0.421   Train Accuracy: 81.46    Test Loss: 0.412   Test Accuracy: 82.15   Time: 0.62sec\n",
      "Epoch: 15   Train Loss: 0.420   Train Accuracy: 81.26    Test Loss: 0.411   Test Accuracy: 82.70   Time: 0.51sec\n",
      "Epoch: 16   Train Loss: 0.420   Train Accuracy: 81.42    Test Loss: 0.414   Test Accuracy: 82.45   Time: 0.52sec\n",
      "Epoch: 17   Train Loss: 0.416   Train Accuracy: 81.72    Test Loss: 0.411   Test Accuracy: 82.65   Time: 0.55sec\n",
      "Epoch: 18   Train Loss: 0.420   Train Accuracy: 81.59    Test Loss: 0.410   Test Accuracy: 82.90   Time: 0.57sec\n",
      "Epoch: 19   Train Loss: 0.416   Train Accuracy: 81.47    Test Loss: 0.404   Test Accuracy: 82.80   Time: 0.58sec\n",
      "Epoch: 20   Train Loss: 0.414   Train Accuracy: 81.74    Test Loss: 0.412   Test Accuracy: 82.60   Time: 0.54sec\n",
      "Epoch: 21   Train Loss: 0.414   Train Accuracy: 82.00    Test Loss: 0.407   Test Accuracy: 82.80   Time: 0.58sec\n",
      "Epoch: 22   Train Loss: 0.413   Train Accuracy: 81.95    Test Loss: 0.405   Test Accuracy: 82.95   Time: 0.57sec\n",
      "Epoch: 23   Train Loss: 0.412   Train Accuracy: 82.06    Test Loss: 0.397   Test Accuracy: 83.45   Time: 0.53sec\n",
      "Epoch: 24   Train Loss: 0.408   Train Accuracy: 82.09    Test Loss: 0.406   Test Accuracy: 83.00   Time: 0.60sec\n",
      "Epoch: 25   Train Loss: 0.409   Train Accuracy: 82.53    Test Loss: 0.399   Test Accuracy: 83.35   Time: 0.57sec\n",
      "Epoch: 26   Train Loss: 0.406   Train Accuracy: 82.15    Test Loss: 0.395   Test Accuracy: 83.85   Time: 0.50sec\n",
      "Epoch: 27   Train Loss: 0.407   Train Accuracy: 82.20    Test Loss: 0.394   Test Accuracy: 84.10   Time: 0.55sec\n",
      "Epoch: 28   Train Loss: 0.405   Train Accuracy: 82.14    Test Loss: 0.391   Test Accuracy: 84.05   Time: 0.61sec\n",
      "Epoch: 29   Train Loss: 0.403   Train Accuracy: 82.58    Test Loss: 0.387   Test Accuracy: 84.35   Time: 0.64sec\n",
      "Epoch: 30   Train Loss: 0.396   Train Accuracy: 82.86    Test Loss: 0.392   Test Accuracy: 83.50   Time: 0.53sec\n",
      "Epoch: 31   Train Loss: 0.397   Train Accuracy: 82.71    Test Loss: 0.380   Test Accuracy: 84.50   Time: 0.64sec\n",
      "Epoch: 32   Train Loss: 0.396   Train Accuracy: 82.85    Test Loss: 0.383   Test Accuracy: 84.55   Time: 0.57sec\n",
      "Epoch: 33   Train Loss: 0.391   Train Accuracy: 82.88    Test Loss: 0.370   Test Accuracy: 84.90   Time: 0.50sec\n",
      "Epoch: 34   Train Loss: 0.385   Train Accuracy: 83.26    Test Loss: 0.372   Test Accuracy: 85.15   Time: 0.52sec\n",
      "Epoch: 35   Train Loss: 0.387   Train Accuracy: 83.44    Test Loss: 0.369   Test Accuracy: 85.25   Time: 0.69sec\n",
      "Epoch: 36   Train Loss: 0.389   Train Accuracy: 82.91    Test Loss: 0.370   Test Accuracy: 85.05   Time: 0.83sec\n",
      "Epoch: 37   Train Loss: 0.381   Train Accuracy: 83.21    Test Loss: 0.365   Test Accuracy: 85.40   Time: 0.67sec\n",
      "Epoch: 38   Train Loss: 0.381   Train Accuracy: 83.42    Test Loss: 0.363   Test Accuracy: 85.30   Time: 0.68sec\n",
      "Epoch: 39   Train Loss: 0.383   Train Accuracy: 83.19    Test Loss: 0.360   Test Accuracy: 85.55   Time: 0.51sec\n",
      "Epoch: 40   Train Loss: 0.381   Train Accuracy: 83.55    Test Loss: 0.360   Test Accuracy: 85.80   Time: 0.52sec\n",
      "Epoch: 41   Train Loss: 0.376   Train Accuracy: 83.60    Test Loss: 0.357   Test Accuracy: 85.45   Time: 0.55sec\n",
      "Epoch: 42   Train Loss: 0.374   Train Accuracy: 83.94    Test Loss: 0.357   Test Accuracy: 85.80   Time: 0.59sec\n",
      "Epoch: 43   Train Loss: 0.375   Train Accuracy: 83.65    Test Loss: 0.355   Test Accuracy: 85.90   Time: 0.58sec\n",
      "Epoch: 44   Train Loss: 0.375   Train Accuracy: 83.65    Test Loss: 0.362   Test Accuracy: 85.80   Time: 0.69sec\n",
      "Epoch: 45   Train Loss: 0.370   Train Accuracy: 83.72    Test Loss: 0.350   Test Accuracy: 86.10   Time: 0.65sec\n",
      "Epoch: 46   Train Loss: 0.373   Train Accuracy: 83.70    Test Loss: 0.351   Test Accuracy: 85.15   Time: 0.59sec\n",
      "Epoch: 47   Train Loss: 0.369   Train Accuracy: 83.72    Test Loss: 0.354   Test Accuracy: 84.90   Time: 0.53sec\n",
      "Epoch: 48   Train Loss: 0.371   Train Accuracy: 83.64    Test Loss: 0.349   Test Accuracy: 85.75   Time: 0.61sec\n",
      "Epoch: 49   Train Loss: 0.369   Train Accuracy: 84.00    Test Loss: 0.347   Test Accuracy: 85.90   Time: 0.50sec\n",
      "Epoch: 50   Train Loss: 0.363   Train Accuracy: 84.03    Test Loss: 0.351   Test Accuracy: 86.05   Time: 0.57sec\n",
      "Epoch: 51   Train Loss: 0.361   Train Accuracy: 84.26    Test Loss: 0.345   Test Accuracy: 85.95   Time: 0.64sec\n",
      "Epoch: 52   Train Loss: 0.364   Train Accuracy: 83.96    Test Loss: 0.343   Test Accuracy: 86.05   Time: 0.81sec\n",
      "Epoch: 53   Train Loss: 0.361   Train Accuracy: 84.29    Test Loss: 0.343   Test Accuracy: 85.90   Time: 0.50sec\n",
      "Epoch: 54   Train Loss: 0.358   Train Accuracy: 84.26    Test Loss: 0.341   Test Accuracy: 86.20   Time: 0.62sec\n",
      "Epoch: 55   Train Loss: 0.363   Train Accuracy: 84.40    Test Loss: 0.343   Test Accuracy: 86.20   Time: 0.60sec\n",
      "Epoch: 56   Train Loss: 0.365   Train Accuracy: 84.41    Test Loss: 0.342   Test Accuracy: 86.25   Time: 0.62sec\n",
      "Epoch: 57   Train Loss: 0.357   Train Accuracy: 84.61    Test Loss: 0.339   Test Accuracy: 86.35   Time: 0.57sec\n",
      "Epoch: 58   Train Loss: 0.361   Train Accuracy: 84.39    Test Loss: 0.339   Test Accuracy: 86.00   Time: 0.66sec\n",
      "Epoch: 59   Train Loss: 0.359   Train Accuracy: 84.50    Test Loss: 0.341   Test Accuracy: 86.30   Time: 0.60sec\n",
      "Epoch: 60   Train Loss: 0.361   Train Accuracy: 84.50    Test Loss: 0.340   Test Accuracy: 85.90   Time: 0.60sec\n",
      "Epoch: 61   Train Loss: 0.358   Train Accuracy: 84.51    Test Loss: 0.340   Test Accuracy: 85.80   Time: 0.53sec\n",
      "Epoch: 62   Train Loss: 0.359   Train Accuracy: 84.49    Test Loss: 0.338   Test Accuracy: 86.05   Time: 0.51sec\n",
      "Epoch: 63   Train Loss: 0.358   Train Accuracy: 84.47    Test Loss: 0.336   Test Accuracy: 86.15   Time: 0.63sec\n",
      "Epoch: 64   Train Loss: 0.357   Train Accuracy: 84.69    Test Loss: 0.339   Test Accuracy: 86.40   Time: 0.69sec\n",
      "Epoch: 65   Train Loss: 0.358   Train Accuracy: 84.50    Test Loss: 0.341   Test Accuracy: 86.10   Time: 0.59sec\n",
      "Epoch: 66   Train Loss: 0.355   Train Accuracy: 84.70    Test Loss: 0.337   Test Accuracy: 85.75   Time: 0.58sec\n",
      "Epoch: 67   Train Loss: 0.353   Train Accuracy: 84.95    Test Loss: 0.343   Test Accuracy: 86.00   Time: 0.58sec\n",
      "Epoch: 68   Train Loss: 0.355   Train Accuracy: 84.72    Test Loss: 0.334   Test Accuracy: 86.00   Time: 0.63sec\n",
      "Epoch: 69   Train Loss: 0.351   Train Accuracy: 85.05    Test Loss: 0.335   Test Accuracy: 86.45   Time: 0.53sec\n",
      "Epoch: 70   Train Loss: 0.358   Train Accuracy: 84.46    Test Loss: 0.337   Test Accuracy: 86.60   Time: 0.53sec\n",
      "Epoch: 71   Train Loss: 0.352   Train Accuracy: 84.80    Test Loss: 0.336   Test Accuracy: 86.65   Time: 0.57sec\n",
      "Epoch: 72   Train Loss: 0.351   Train Accuracy: 84.81    Test Loss: 0.331   Test Accuracy: 86.95   Time: 0.59sec\n",
      "Epoch: 73   Train Loss: 0.352   Train Accuracy: 84.88    Test Loss: 0.329   Test Accuracy: 86.95   Time: 0.67sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74   Train Loss: 0.347   Train Accuracy: 85.10    Test Loss: 0.332   Test Accuracy: 86.90   Time: 0.66sec\n",
      "Epoch: 75   Train Loss: 0.351   Train Accuracy: 85.00    Test Loss: 0.336   Test Accuracy: 86.35   Time: 0.54sec\n",
      "Epoch: 76   Train Loss: 0.349   Train Accuracy: 85.40    Test Loss: 0.331   Test Accuracy: 86.50   Time: 0.68sec\n",
      "Epoch: 77   Train Loss: 0.347   Train Accuracy: 85.39    Test Loss: 0.327   Test Accuracy: 86.95   Time: 0.80sec\n",
      "Epoch: 78   Train Loss: 0.350   Train Accuracy: 84.85    Test Loss: 0.335   Test Accuracy: 86.95   Time: 0.56sec\n",
      "Epoch: 79   Train Loss: 0.345   Train Accuracy: 85.49    Test Loss: 0.330   Test Accuracy: 87.10   Time: 0.60sec\n",
      "Epoch: 80   Train Loss: 0.348   Train Accuracy: 84.96    Test Loss: 0.330   Test Accuracy: 86.80   Time: 0.71sec\n",
      "Epoch: 81   Train Loss: 0.349   Train Accuracy: 84.85    Test Loss: 0.331   Test Accuracy: 87.35   Time: 0.65sec\n",
      "Epoch: 82   Train Loss: 0.348   Train Accuracy: 85.05    Test Loss: 0.329   Test Accuracy: 87.30   Time: 0.54sec\n",
      "Epoch: 83   Train Loss: 0.346   Train Accuracy: 85.30    Test Loss: 0.327   Test Accuracy: 87.25   Time: 0.62sec\n",
      "Epoch: 84   Train Loss: 0.348   Train Accuracy: 85.19    Test Loss: 0.329   Test Accuracy: 86.90   Time: 0.53sec\n",
      "Epoch: 85   Train Loss: 0.347   Train Accuracy: 85.29    Test Loss: 0.328   Test Accuracy: 87.10   Time: 0.65sec\n",
      "Epoch: 86   Train Loss: 0.344   Train Accuracy: 85.21    Test Loss: 0.328   Test Accuracy: 86.95   Time: 0.51sec\n",
      "Epoch: 87   Train Loss: 0.348   Train Accuracy: 85.12    Test Loss: 0.326   Test Accuracy: 87.15   Time: 0.58sec\n",
      "Epoch: 88   Train Loss: 0.343   Train Accuracy: 85.28    Test Loss: 0.332   Test Accuracy: 86.60   Time: 0.62sec\n",
      "Epoch: 89   Train Loss: 0.342   Train Accuracy: 85.42    Test Loss: 0.331   Test Accuracy: 87.10   Time: 0.52sec\n",
      "Epoch: 90   Train Loss: 0.341   Train Accuracy: 85.44    Test Loss: 0.327   Test Accuracy: 87.40   Time: 0.55sec\n",
      "Epoch: 91   Train Loss: 0.345   Train Accuracy: 85.17    Test Loss: 0.328   Test Accuracy: 87.45   Time: 0.52sec\n",
      "Epoch: 92   Train Loss: 0.341   Train Accuracy: 85.30    Test Loss: 0.329   Test Accuracy: 86.85   Time: 0.62sec\n",
      "Epoch: 93   Train Loss: 0.345   Train Accuracy: 85.14    Test Loss: 0.325   Test Accuracy: 87.40   Time: 0.70sec\n",
      "Epoch: 94   Train Loss: 0.335   Train Accuracy: 86.06    Test Loss: 0.323   Test Accuracy: 87.50   Time: 0.66sec\n",
      "Epoch: 95   Train Loss: 0.344   Train Accuracy: 85.11    Test Loss: 0.325   Test Accuracy: 87.40   Time: 0.53sec\n",
      "Epoch: 96   Train Loss: 0.345   Train Accuracy: 85.03    Test Loss: 0.331   Test Accuracy: 86.95   Time: 0.68sec\n",
      "Epoch: 97   Train Loss: 0.341   Train Accuracy: 85.65    Test Loss: 0.325   Test Accuracy: 87.20   Time: 0.64sec\n",
      "Epoch: 98   Train Loss: 0.341   Train Accuracy: 85.30    Test Loss: 0.326   Test Accuracy: 87.80   Time: 0.62sec\n",
      "Epoch: 99   Train Loss: 0.347   Train Accuracy: 85.29    Test Loss: 0.324   Test Accuracy: 87.50   Time: 0.52sec\n",
      "Epoch: 100   Train Loss: 0.339   Train Accuracy: 85.78    Test Loss: 0.326   Test Accuracy: 87.20   Time: 0.52sec\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import time\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "a_tr_loss = np.zeros([num_epoch])\n",
    "a_tr_accuracy = np.zeros([num_epoch])\n",
    "a_ts_loss = np.zeros([num_epoch])\n",
    "a_ts_accuracy = np.zeros([num_epoch])\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    correct = 0 # initialize error counter\n",
    "    total = 0 # initialize total counter\n",
    "    batch_loss = []\n",
    "    tic = time.time()\n",
    "    \n",
    "    model1.train() # put model in training mode\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_dl):\n",
    "        x_batch,y_batch = data\n",
    "        y_batch = y_batch.view(-1,1)\n",
    "        out = model1(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out.clamp(0,1),y_batch.type(torch.float))\n",
    "        batch_loss.append(loss.item())\n",
    "        # Zero gradients\n",
    "        opt.zero_grad()\n",
    "        # Compute gradients using back propagation\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        predicted = out.clamp(0,1).round().type(torch.long)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "    a_tr_loss[epoch] = np.mean(batch_loss) # Compute average loss over epoch\n",
    "    a_tr_accuracy[epoch] = 100*correct/total\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_loss = []\n",
    "    model1.eval() # put model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            images, labels = data\n",
    "            labels = labels.view(-1,1)\n",
    "            out = model1(images)\n",
    "            batch_loss.append(criterion(out.clamp(0,1),labels.type(torch.float)).item())\n",
    "            predicted = out.clamp(0,1).round().type(torch.long)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    a_ts_loss[epoch] = np.mean(batch_loss)\n",
    "    a_ts_accuracy[epoch] = 100*correct/total\n",
    "    elapsed = time.time()-tic\n",
    "    \n",
    "    # Print details every print_mod epoch\n",
    "    print('Epoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "          +'Train Accuracy: {0:.2f}    Test Loss: {1:.3f}   '.format(a_tr_accuracy[epoch], a_ts_loss[epoch])\n",
    "          +'Test Accuracy: {0:.2f}   '.format(a_ts_accuracy[epoch])\n",
    "          +'Time: {0:.2f}sec'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training auc:  0.9199856429850293\n"
     ]
    }
   ],
   "source": [
    "# compute the training accuracy\n",
    "with torch.no_grad():\n",
    "    predict = model1(torch.Tensor(Xtr_standardized)).detach().numpy().ravel()\n",
    "    \n",
    "\n",
    "auc = roc_auc_score(ytr_standardized,predict)\n",
    "print('training auc: ',auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model: you must use the .pth format for pytorch models!\n",
    "model_savepath = 'model1.pth'\n",
    "\n",
    "# To save a PyTorch model, we first pass an input through the model, \n",
    "# and then save the \"trace\". \n",
    "# For this purpose, we can use any input. \n",
    "# We will create a random input with the proper dimension.\n",
    "x = torch.randn(d_in) # random input\n",
    "x = x[None,:] # add singleton batch index\n",
    "with torch.no_grad():\n",
    "    traced_cell = torch.jit.trace(model1, (x))\n",
    "\n",
    "# Now we save the trace\n",
    "torch.jit.save(traced_cell, model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training auc =  0.9199856429850293\n",
      "test label confidences saved in yts_hat_pytorch.csv\n"
     ]
    }
   ],
   "source": [
    "# generate kaggle submission file using the validation script\n",
    "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
